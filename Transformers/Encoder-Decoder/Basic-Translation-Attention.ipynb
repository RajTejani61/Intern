{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f683a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\torch_env\\lib\\site-packages\\torchtext\\vocab\\__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "c:\\ProgramData\\anaconda3\\envs\\torch_env\\lib\\site-packages\\torchtext\\utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchtext\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import nltk\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d1362e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2cb0276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# English -> French pairs\n",
    "pairs = [\n",
    "    (\"i am a student\", \"je suis un étudiant\"),\n",
    "    (\"he is a teacher\", \"il est un professeur\"),\n",
    "    (\"she likes pizza\", \"elle aime la pizza\"),\n",
    "    (\"we are friends\", \"nous sommes amis\"),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2e53514",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return [token.lower() for token in nltk.tokenize.word_tokenize(text) if token.isalnum()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7993d5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab = {\"<pad>\": 0, \"<sos>\":1, \"<eos>\":2}\n",
    "trg_vocab = {\"<pad>\": 0, \"<sos>\":1, \"<eos>\":2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260dcca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for en, fr in pairs:\n",
    "    for token in tokenize(en):\n",
    "        if token not in src_vocab:\n",
    "            src_vocab[token] = len(src_vocab)\n",
    "    for token in tokenize(fr):\n",
    "        if token not in trg_vocab:\n",
    "            trg_vocab[token] = len(trg_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34e52a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_trg_vocab = {v: k for k, v in trg_vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8533c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(sentence, vocab):\n",
    "    return [vocab[\"<sos>\"]] + [vocab[token] for token in tokenize(sentence)] + [vocab[\"<eos>\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a1e735",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(encode(en, src_vocab), encode(fr, trg_vocab)) for en, fr in pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e797d82f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([1, 3, 4, 5, 6, 2], [1, 3, 4, 5, 6, 2]),\n",
       " ([1, 7, 8, 5, 9, 2], [1, 7, 8, 5, 9, 2]),\n",
       " ([1, 10, 11, 12, 2], [1, 10, 11, 12, 13, 2]),\n",
       " ([1, 13, 14, 15, 2], [1, 14, 15, 16, 2])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eace1b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.rnn = nn.GRU(embedding_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        embedded = self.embedding(src)\n",
    "        output, hidden = self.rnn(embedded)\n",
    "        #  output : [batch_size, seq_len, hidden_dim * 2]\n",
    "        #  hidden : [n_layers * 2, batch_size, hidden_dim]\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87292c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hidden_dim, dec_hidden_dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(enc_hidden_dim * 2 + dec_hidden_dim, dec_hidden_dim)\n",
    "        self.v = nn.Linear(dec_hidden_dim, 1, bias=False)\n",
    "        \n",
    "    def forward(self, hidden, encoder_output):\n",
    "        # hidden : [n_layers, batch_size, dec_hidden_dim]\n",
    "        # encoder_output : [batch_size, src_len, enc_hidden_dim * 2]\n",
    "        \n",
    "        # batch_size = encoder_output.shape[-1]\n",
    "        src_len = encoder_output.shape[1]\n",
    "        \n",
    "        # repeat decoder hidden state src_len times\n",
    "        hidden = hidden[-1].unsqueeze(1).repeat(1, src_len, 1)\n",
    "        # hidden : [batch_size, src_len, dec_hidden_dim]\n",
    "        # encoder_output : [batch_size, src_len, enc_hidden_dim * 2]\n",
    "        \n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_output), dim=2)))\n",
    "        # energy : [batch_size, src_len, dec_hidden_dim]\n",
    "        \n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        # attention : [batch_size, src_len]\n",
    "        \n",
    "        return nn.functional.softmax(attention, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "413431d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, embedding_dim, enc_hidden_dim, dec_hidden_dim, attention):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim # [size of vocab]\n",
    "        self.attention = attention\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dim, embedding_dim)\n",
    "        self.rnn = nn.GRU(enc_hidden_dim*2 + embedding_dim, dec_hidden_dim, batch_first=True)\n",
    "        self.fc_out = nn.Linear(enc_hidden_dim*2 + dec_hidden_dim + embedding_dim, output_dim)\n",
    "        \n",
    "    def forward(self, input, hidden, encoder_output):\n",
    "        # input : [batch_size]\n",
    "        # hidden : [n_layers, batch_size, dec_hidden_dim]\n",
    "        # encoder_output : [batch_size, src_len, enc_hidden_dim * 2]\n",
    "        \n",
    "        embedded = self.embedding(input).unsqueeze(1)\n",
    "        # embedded : [batch_size, 1, embedding_dim]\n",
    "        \n",
    "        attention = self.attention(hidden, encoder_output)\n",
    "        # attention : [batch_size, src_len]\n",
    "        \n",
    "        attention = attention.unsqueeze(1)\n",
    "        # attention : [batch_size, 1, src_len]\n",
    "        \n",
    "        # encoder_output : [batch_size, src_len, enc_hidden_dim * 2]\n",
    "        \n",
    "        weighted = torch.bmm(attention, encoder_output)\n",
    "        # weighted : [batch_size, 1, enc_hidden_dim * 2]\n",
    "        \n",
    "        rnn_input = torch.cat((embedded, weighted), dim=2)\n",
    "        # rnn_input : [batch_size, 1, enc_hidden_dim * 2 + embedding_dim]\n",
    "        \n",
    "        output, hidden = self.rnn(rnn_input, hidden[-1].unsqueeze(0))\n",
    "        # output : [batch_size, 1, dec_hidden_dim]\n",
    "        # hidden : [n_layers, batch_size, dec_hidden_dim]\n",
    "        \n",
    "        output = output.squeeze(1)\n",
    "        # output : [batch_size, dec_hidden_dim]\n",
    "        embedded = embedded.squeeze(1)\n",
    "        # embedded : [batch_size, embedding_dim]\n",
    "        weighted = weighted.squeeze(1)\n",
    "        # weighted : [batch_size, enc_hidden_dim * 2]\n",
    "        output = self.fc_out(torch.cat((output, weighted, embedded), dim=1))\n",
    "        # output : [batch_size, output_dim]\n",
    "        \n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd92d0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "    \n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        # src : [batch_size, src_len]\n",
    "        # trg : [batch_size, trg_len]\n",
    "\n",
    "        batch_size = src.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        enc_outputs, hidden = self.encoder(src)\n",
    "        hidden = hidden[::2] + hidden[1::2] \n",
    "        \n",
    "        input = trg[:, 0]\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden = self.decoder(input, hidden, enc_outputs)\n",
    "            outputs[t] = output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1)\n",
    "            input = trg[:, t] if teacher_force else top1\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa468ab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 17)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(src_vocab), len(trg_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb934598",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(src_vocab)\n",
    "OUTPUT_DIM = len(trg_vocab)\n",
    "ENC_EMB_DIM = DEC_EMB_DIM = 32\n",
    "HID_DIM = 64\n",
    "\n",
    "attn = Attention(HID_DIM, HID_DIM)\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, HID_DIM, attn)\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc93a652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100, Loss: 1.469619\n",
      "Epoch 20/100, Loss: 0.284535\n",
      "Epoch 30/100, Loss: 0.047490\n",
      "Epoch 40/100, Loss: 0.020250\n",
      "Epoch 50/100, Loss: 0.011936\n",
      "Epoch 60/100, Loss: 0.007950\n",
      "Epoch 70/100, Loss: 0.005689\n",
      "Epoch 80/100, Loss: 0.004341\n",
      "Epoch 90/100, Loss: 0.003446\n",
      "Epoch 100/100, Loss: 0.002814\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for src, trg in data:\n",
    "        src = torch.tensor(src).unsqueeze(0).to(device)  # [1, src_len]\n",
    "        trg = torch.tensor(trg).unsqueeze(0).to(device)  # [1, trg_len]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg)\n",
    "\n",
    "        output_dim = output.shape[-1]\n",
    "\n",
    "        output = output[1:].reshape(-1, output_dim)  # [N, C]\n",
    "        trg = trg[:, 1:].reshape(-1)                    # [N]\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:    \n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {epoch_loss/len(data):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "14c24a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    src = torch.tensor(encode(sentence, src_vocab)).unsqueeze(0).to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs, hidden = enc(src)\n",
    "        hidden = hidden[::2] + hidden[1::2]\n",
    "        input = torch.tensor([trg_vocab[\"<sos>\"]]).to(device)\n",
    "        result = []\n",
    "        for _ in range(10):\n",
    "            output, hidden = dec(input, hidden, encoder_outputs)\n",
    "            top1 = output.argmax(1).item()\n",
    "            if top1 == trg_vocab[\"<eos>\"]:\n",
    "                break\n",
    "            result.append(inv_trg_vocab[top1])\n",
    "            input = torch.tensor([top1]).to(device)\n",
    "    return \" \".join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90105ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Translations:\n",
      "i am a student -> je suis un étudiant (expected: je suis un étudiant)\n",
      "he is a teacher -> il est un professeur (expected: il est un professeur)\n",
      "she likes pizza -> elle aime la pizza (expected: elle aime la pizza)\n",
      "we are friends -> nous sommes amis (expected: nous sommes amis)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTranslations:\")\n",
    "for en, fr in pairs:\n",
    "    translation = translate(en)\n",
    "    print(f\"{en} -> {translation} (expected: {fr})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a0d82c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
